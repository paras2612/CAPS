# -*- coding: utf-8 -*-
"""Preprocessing Hydrological Data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J21-8grRLW9Q2WgAvOVXJ9slc9g0h8P-
"""

#Reading the output columns from output.rch for subbasin 68
import re
with open('/content/drive/MyDrive/RA-Hydrological/output.hru') as f:
  lines = f.readlines()
output_dict = {}
for i in range(9,len(lines)):
  if(lines[i].split()[2]=='68'):
    print(lines[i].split())# and int(lines[i].split()[2])<=682715):# and int(lines[i].split()[2])<=690000):
    if(lines[i].split()[1] in output_dict.keys()):
      try:
        area=float(re.findall("\.\d\d\d\d\dE\+\d\d",lines[i])[0])
      except:
        area=float(re.findall("\.\d\d\d\d\dE\-\d\d",lines[i])[0])
      wyld=float(lines[i].split()[-1])
      output_dict[lines[i].split()[1]].extend([area,wyld])
    else:
      try:
        area=float(re.findall("\.\d\d\d\d\dE\+\d\d",lines[i])[0])
      except:
        area=float(re.findall("\.\d\d\d\d\dE\-\d\d",lines[i])[0])
      wyld=float(lines[i].split()[-1])
      temp = [area,wyld]
      output_dict[lines[i].split()[1]] = temp

import pandas as pd
hru_num=[]
area=[]
wyld=[]
a = list(output_dict.keys())
for i in a:
  for j in range(len(output_dict[i])):
    if(j%2==0):
      hru_num.append(i)
      area.append(output_dict[i][j])
    elif(j%2!=0):
      wyld.append(output_dict[i][j])

1981950//2715

1982680-1981950

data_dict = {"HRU_NUM":hru_num,"AREAkm2":area,"WYLDmm":wyld}
output=pd.DataFrame.from_dict(data_dict)
#output = output[['HRU_NUM','AREAkm2']]

output

filteredoutput= output.groupby('HRU_NUM').mean()
filteredoutput

import os
#Reading Input Files
path = '/content/drive/MyDrive/RA-Hydrological/Sub68/Spatial_Input_Data/'
feature_dict={}
files = os.listdir(path)
a=0
b=0
c=0
d=0
for f in files:
  if(f[-2:]=='gt'):
    cn2_list=[]
    with open(path+f) as f1:
      lines = f1.readlines()
      #Extract feature names (CN2) and values 
      if(lines[10].strip().split("|")[1].split(":")[0].strip() in feature_dict.keys()):
        feature_dict[lines[10].strip().split("|")[1].split(":")[0].strip()].extend([float(lines[10].strip().split("|")[0])])
      else:
        feature_dict[lines[10].strip().split("|")[1].split(":")[0].strip()]=[float(lines[10].strip().split("|")[0])]
    a+=1
    #lines[3:8],lines[9:14],lines[15:17],lines[18:23],lines[24:27],lines[29] all feature locations
  
  elif(f[-2:]=='gw'):
    with open(path+f) as f1:
      lines = f1.readlines()
      #Extract feature names (GW_DELAY,ALPHA_BF,GWQMN,GW_REVAP,REVAPMN,RCHRG_DP,GW_SPYLD,ALPHA_BF_D) and values 
      if(lines[3].strip().split("|")[1].split(":")[0].strip() in feature_dict.keys()):
        feature_dict[lines[3].strip().split("|")[1].split(":")[0].strip()].extend([float(lines[3].strip().split("|")[0])])
      else:
        feature_dict[lines[3].strip().split("|")[1].split(":")[0].strip()]=[float(lines[3].strip().split("|")[0])]
      if(lines[4].strip().split("|")[1].split(":")[0].strip() in feature_dict.keys()):
        feature_dict[lines[4].strip().split("|")[1].split(":")[0].strip()].extend([float(lines[4].strip().split("|")[0])])
      else:
        feature_dict[lines[4].strip().split("|")[1].split(":")[0].strip()]=[float(lines[4].strip().split("|")[0])]
      if(lines[5].strip().split("|")[1].split(":")[0].strip() in feature_dict.keys()):
        feature_dict[lines[5].strip().split("|")[1].split(":")[0].strip()].extend([float(lines[5].strip().split("|")[0])])
      else:
        feature_dict[lines[5].strip().split("|")[1].split(":")[0].strip()]=[float(lines[5].strip().split("|")[0])]
      if(lines[6].strip().split("|")[1].split(":")[0].strip() in feature_dict.keys()):
        feature_dict[lines[6].strip().split("|")[1].split(":")[0].strip()].extend([float(lines[6].strip().split("|")[0])])
      else:
        feature_dict[lines[6].strip().split("|")[1].split(":")[0].strip()]=[float(lines[6].strip().split("|")[0])]
      if(lines[7].strip().split("|")[1].split(":")[0].strip() in feature_dict.keys()):
        feature_dict[lines[7].strip().split("|")[1].split(":")[0].strip()].extend([float(lines[7].strip().split("|")[0])])
      else:
        feature_dict[lines[7].strip().split("|")[1].split(":")[0].strip()]=[float(lines[7].strip().split("|")[0])]
      if(lines[8].strip().split("|")[1].split(":")[0].strip() in feature_dict.keys()):
        feature_dict[lines[8].strip().split("|")[1].split(":")[0].strip()].extend([float(lines[8].strip().split("|")[0])])
      else:
        feature_dict[lines[8].strip().split("|")[1].split(":")[0].strip()]=[float(lines[8].strip().split("|")[0])]
      if(lines[10].strip().split("|")[1].split(":")[0].strip() in feature_dict.keys()):
        feature_dict[lines[10].strip().split("|")[1].split(":")[0].strip()].extend([float(lines[10].strip().split("|")[0])])
      else:
        feature_dict[lines[10].strip().split("|")[1].split(":")[0].strip()]=[float(lines[10].strip().split("|")[0])]
      if(lines[16].strip().split("|")[1].split(":")[0].strip() in feature_dict.keys()):
        feature_dict[lines[16].strip().split("|")[1].split(":")[0].strip()].extend([float(lines[16].strip().split("|")[0])])
      else:
        feature_dict[lines[16].strip().split("|")[1].split(":")[0].strip()]=[float(lines[16].strip().split("|")[0])]
      b+=1
  elif(f[-2:]=="ru"):
    with open(path+f) as f1:
      lines = f1.readlines()
      #Extract feature names (HRU_FR, DIS_STREAM,SLSUBBSN, HRU_SLP, OV_N, LAT_TTIME, SLSOIL, CANMX, ESCO, EPCO, DEP_IMP, CF, CFH, CFDEC, SURLAG, R2ADJ) and values
      if(lines[1].strip().split("|")[1].split(":")[0].strip() in feature_dict.keys()):
        feature_dict[lines[1].strip().split("|")[1].split(":")[0].strip()].extend([float(lines[1].strip().split("|")[0])])
      else:
        feature_dict[lines[1].strip().split("|")[1].split(":")[0].strip()]=[float(lines[1].strip().split("|")[0])]
      if(lines[28].strip().split("|")[1].split(":")[0].strip() in feature_dict.keys()):
        feature_dict[lines[28].strip().split("|")[1].split(":")[0].strip()].extend([float(lines[28].strip().split("|")[0])])
      else:
        feature_dict[lines[28].strip().split("|")[1].split(":")[0].strip()]=[float(lines[28].strip().split("|")[0])]
      if(lines[2].strip().split("|")[1].split(":")[0].strip() in feature_dict.keys()):
        feature_dict[lines[2].strip().split("|")[1].split(":")[0].strip()].extend([float(lines[2].strip().split("|")[0])])
      else:
        feature_dict[lines[2].strip().split("|")[1].split(":")[0].strip()]=[float(lines[2].strip().split("|")[0])]
      if(lines[3].strip().split("|")[1].split(":")[0].strip() in feature_dict.keys()):
        feature_dict[lines[3].strip().split("|")[1].split(":")[0].strip()].extend([float(lines[3].strip().split("|")[0])])
      else:
        feature_dict[lines[3].strip().split("|")[1].split(":")[0].strip()]=[float(lines[3].strip().split("|")[0])]
      if(lines[4].strip().split("|")[1].split(":")[0].strip() in feature_dict.keys()):
        feature_dict[lines[4].strip().split("|")[1].split(":")[0].strip()].extend([float(lines[4].strip().split("|")[0])])
      else:
        feature_dict[lines[4].strip().split("|")[1].split(":")[0].strip()]=[float(lines[4].strip().split("|")[0])]
      if(lines[5].strip().split("|")[1].split(":")[0].strip() in feature_dict.keys()):
        feature_dict[lines[5].strip().split("|")[1].split(":")[0].strip()].extend([float(lines[5].strip().split("|")[0])])
      else:
        feature_dict[lines[5].strip().split("|")[1].split(":")[0].strip()]=[float(lines[5].strip().split("|")[0])]
      if(lines[7].strip().split("|")[1].split(":")[0].strip() in feature_dict.keys()):
        feature_dict[lines[7].strip().split("|")[1].split(":")[0].strip()].extend([float(lines[7].strip().split("|")[0])])
      else:
        feature_dict[lines[7].strip().split("|")[1].split(":")[0].strip()]=[float(lines[7].strip().split("|")[0])]
      if(lines[8].strip().split("|")[1].split(":")[0].strip() in feature_dict.keys()):
        feature_dict[lines[8].strip().split("|")[1].split(":")[0].strip()].extend([float(lines[8].strip().split("|")[0])])
      else:
        feature_dict[lines[8].strip().split("|")[1].split(":")[0].strip()]=[float(lines[8].strip().split("|")[0])]
      if(lines[9].strip().split("|")[1].split(":")[0].strip() in feature_dict.keys()):
        feature_dict[lines[9].strip().split("|")[1].split(":")[0].strip()].extend([float(lines[9].strip().split("|")[0])])
      else:
        feature_dict[lines[9].strip().split("|")[1].split(":")[0].strip()]=[float(lines[9].strip().split("|")[0])]
      if(lines[10].strip().split("|")[1].split(":")[0].strip() in feature_dict.keys()):
        feature_dict[lines[10].strip().split("|")[1].split(":")[0].strip()].extend([float(lines[10].strip().split("|")[0])])
      else:
        feature_dict[lines[10].strip().split("|")[1].split(":")[0].strip()]=[float(lines[10].strip().split("|")[0])]
      if(lines[23].strip().split("|")[1].split(":")[0].strip() in feature_dict.keys()):
        feature_dict[lines[23].strip().split("|")[1].split(":")[0].strip()].extend([float(lines[23].strip().split("|")[0])])
      else:
        feature_dict[lines[23].strip().split("|")[1].split(":")[0].strip()]=[float(lines[23].strip().split("|")[0])]
      if(lines[29].strip().split("|")[1].split(":")[0].strip() in feature_dict.keys()):
        feature_dict[lines[29].strip().split("|")[1].split(":")[0].strip()].extend([float(lines[29].strip().split("|")[0])])
      else:
        feature_dict[lines[29].strip().split("|")[1].split(":")[0].strip()]=[float(lines[29].strip().split("|")[0])]
      if(lines[30].strip().split("|")[1].split(":")[0].strip() in feature_dict.keys()):
        feature_dict[lines[30].strip().split("|")[1].split(":")[0].strip()].extend([float(lines[30].strip().split("|")[0])])
      else:
        feature_dict[lines[30].strip().split("|")[1].split(":")[0].strip()]=[float(lines[30].strip().split("|")[0])]
      if(lines[31].strip().split("|")[1].split(":")[0].strip() in feature_dict.keys()):
        feature_dict[lines[31].strip().split("|")[1].split(":")[0].strip()].extend([float(lines[31].strip().split("|")[0])])
      else:
        feature_dict[lines[31].strip().split("|")[1].split(":")[0].strip()]=[float(lines[31].strip().split("|")[0])]
      if(lines[43].strip().split("|")[1].split(":")[0].strip() in feature_dict.keys()):
        feature_dict[lines[43].strip().split("|")[1].split(":")[0].strip()].extend([float(lines[43].strip().split("|")[0])])
      else:
        feature_dict[lines[43].strip().split("|")[1].split(":")[0].strip()]=[float(lines[43].strip().split("|")[0])]
      if(lines[44].strip().split("|")[1].split(":")[0].strip() in feature_dict.keys()):
        feature_dict[lines[44].strip().split("|")[1].split(":")[0].strip()].extend([float(lines[44].strip().split("|")[0])])
      else:
        feature_dict[lines[44].strip().split("|")[1].split(":")[0].strip()]=[float(lines[44].strip().split("|")[0])]
      c+=1
  elif(f[-2:]=="ol"):
    with open(path+f) as f1:
      lines = f1.readlines()
      #Extract features (each parameter has 3 layers, please include all the three layers, you can use Depth1, Depth2, Depth3 to differ them): Crack volume potential of soil, Depth, Bulk Density Moist, Ave. AW Incl. Rock Frag, Ksat. and values
      if(lines[5].strip().split(":")[0].strip() in feature_dict.keys()):
        feature_dict[lines[5].split(":")[0].strip()].extend([float(lines[5].strip().split(":")[1])])
      else:
        feature_dict[lines[5].split(":")[0].strip()]=[float(lines[5].strip().split(":")[1])]
      if(lines[7].strip().split(":")[0].strip()+"1" in feature_dict.keys()):
        feature_dict[lines[7].split(":")[0].strip()+"1"].extend([float(lines[7].strip().split(":")[1].split()[0])])
      else:
        feature_dict[lines[7].split(":")[0].strip()+"1"]=[float(lines[7].strip().split(":")[1].split()[0])]
      try:
        if(lines[7].strip().split(":")[0].strip()+"2" in feature_dict.keys()):
          feature_dict[lines[7].split(":")[0].strip()+"2"].extend([float(lines[7].strip().split(":")[1].split()[1])])
        else:
          feature_dict[lines[7].split(":")[0].strip()+"2"]=[float(lines[7].strip().split(":")[1].split()[1])]
      except:
        if(lines[7].strip().split(":")[0].strip()+"2" in feature_dict.keys()):
          feature_dict[lines[7].split(":")[0].strip()+"2"].extend([float(0.0)])
        else:
          feature_dict[lines[7].split(":")[0].strip()+"2"]=[float(0.0)]
      try:
        if(lines[7].strip().split(":")[0].strip()+"3" in feature_dict.keys()):
          feature_dict[lines[7].split(":")[0].strip()+"3"].extend([float(lines[7].strip().split(":")[1].split()[2])])
        else:
          feature_dict[lines[7].split(":")[0].strip()+"3"]=[float(lines[7].strip().split(":")[1].split()[2])]
      except:
        if(lines[7].strip().split(":")[0].strip()+"3" in feature_dict.keys()):
          feature_dict[lines[7].split(":")[0].strip()+"3"].extend([float(0.0)])
        else:
          feature_dict[lines[7].split(":")[0].strip()+"3"]=[float(0.0)]
      if(lines[8].strip().split(":")[0].strip()+"1" in feature_dict.keys()):
        feature_dict[lines[8].split(":")[0].strip()+"1"].extend([float(lines[8].strip().split(":")[1].split()[0])])
      else:
        feature_dict[lines[8].split(":")[0].strip()+"1"]=[float(lines[8].strip().split(":")[1].split()[0])]
      try:
        if(lines[8].strip().split(":")[0].strip()+"2" in feature_dict.keys()):
          feature_dict[lines[8].split(":")[0].strip()+"2"].extend([float(lines[8].strip().split(":")[1].split()[1])])
        else:
          feature_dict[lines[8].split(":")[0].strip()+"2"]=[float(lines[8].strip().split(":")[1].split()[1])]
      except:
        if(lines[8].strip().split(":")[0].strip()+"2" in feature_dict.keys()):
          feature_dict[lines[8].split(":")[0].strip()+"2"].extend([float(0.0)])
        else:
          feature_dict[lines[8].split(":")[0].strip()+"2"]=[float(0.0)]
      try:
        if(lines[8].strip().split(":")[0].strip()+"3" in feature_dict.keys()):
          feature_dict[lines[8].split(":")[0].strip()+"3"].extend([float(lines[8].strip().split(":")[1].split()[2])])
        else:
          feature_dict[lines[8].split(":")[0].strip()+"3"]=[float(lines[8].strip().split(":")[1].split()[2])]
      except:
        if(lines[8].strip().split(":")[0].strip()+"3" in feature_dict.keys()):
          feature_dict[lines[8].split(":")[0].strip()+"3"].extend([float(0.0)])
        else:
          feature_dict[lines[8].split(":")[0].strip()+"3"]=[float(0.0)]
      if(lines[9].strip().split(":")[0].strip()+"1" in feature_dict.keys()):
        feature_dict[lines[9].split(":")[0].strip()+"1"].extend([float(lines[9].strip().split(":")[1].split()[0])])
      else:
        feature_dict[lines[9].split(":")[0].strip()+"1"]=[float(lines[9].strip().split(":")[1].split()[0])]
      try:
        if(lines[9].strip().split(":")[0].strip()+"2" in feature_dict.keys()):
          feature_dict[lines[9].split(":")[0].strip()+"2"].extend([float(lines[9].strip().split(":")[1].split()[1])])
        else:
          feature_dict[lines[9].split(":")[0].strip()+"2"]=[float(lines[9].strip().split(":")[1].split()[1])]
      except:
        if(lines[9].strip().split(":")[0].strip()+"2" in feature_dict.keys()):
          feature_dict[lines[9].split(":")[0].strip()+"2"].extend([float(0.0)])
        else:
          feature_dict[lines[9].split(":")[0].strip()+"2"]=[float(0.0)]
      try:
        if(lines[9].strip().split(":")[0].strip()+"3" in feature_dict.keys()):
          feature_dict[lines[9].split(":")[0].strip()+"3"].extend([float(lines[9].strip().split(":")[1].split()[2])])
        else:
          feature_dict[lines[9].split(":")[0].strip()+"3"]=[float(lines[9].strip().split(":")[1].split()[2])]
      except:
        if(lines[9].strip().split(":")[0].strip()+"3" in feature_dict.keys()):
          feature_dict[lines[9].split(":")[0].strip()+"3"].extend([float(0.0)])
        else:
          feature_dict[lines[9].split(":")[0].strip()+"3"]=[float(0.0)]
      
      if(lines[10].strip().split(":")[0].strip()+"1" in feature_dict.keys()):
        feature_dict[lines[10].split(":")[0].strip()+"1"].extend([float(lines[10].strip().split(":")[1].split()[0])])
      else:
        feature_dict[lines[10].split(":")[0].strip()+"1"]=[float(lines[10].strip().split(":")[1].split()[0])]
      try:
        if(lines[10].strip().split(":")[0].strip()+"2" in feature_dict.keys()):
          feature_dict[lines[10].split(":")[0].strip()+"2"].extend([float(lines[10].strip().split(":")[1].split()[1])])
        else:
          feature_dict[lines[10].split(":")[0].strip()+"2"]=[float(lines[10].strip().split(":")[1].split()[1])]
      except:
        if(lines[10].strip().split(":")[0].strip()+"2" in feature_dict.keys()):
          feature_dict[lines[10].split(":")[0].strip()+"2"].extend([float(0.0)])
        else:
          feature_dict[lines[10].split(":")[0].strip()+"2"]=[float(0.0)]
      try:
        if(lines[10].strip().split(":")[0].strip()+"3" in feature_dict.keys()):
          feature_dict[lines[10].split(":")[0].strip()+"3"].extend([float(lines[10].strip().split(":")[1].split()[2])])
        else:
          feature_dict[lines[10].split(":")[0].strip()+"3"]=[float(lines[10].strip().split(":")[1].split()[2])]
      except:
        if(lines[10].strip().split(":")[0].strip()+"3" in feature_dict.keys()):
          feature_dict[lines[10].split(":")[0].strip()+"3"].extend([float(0.0)])
        else:
          feature_dict[lines[10].split(":")[0].strip()+"3"]=[float(0.0)]  
      d+=1
  print("Done for ",a," mgt files, ",b," gw files, ",c," hru files, and ",d," sol files")

feats=list(feature_dict.keys())
for i in feats:
  print(i,len(feature_dict[i]))

import pandas as pd
#inputdata = pd.DataFrame.from_dict(feature_dict)
#inputdata
#inputdata.to_csv("/content/drive/MyDrive/RA-Hydrological/Sub68/StaticInputData.csv",index=False)

arae=filteredoutput['AREAkm2'].values
wyld=filteredoutput['WYLDmm'].values
inputdata['AREAkm2']=arae
inputdata['WYLDmm']=wyld
alldata = inputdata

inputdata['Depth                [mm]1'].values[0]

alldata

import pandas as pd
from sklearn import preprocessing

x = alldata.values #returns a numpy array
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
df = pd.DataFrame(x_scaled)

df

df.columns=alldata.columns
df

alldata

df.to_csv("/content/drive/MyDrive/RA-Hydrological/Sub68/AllDataAvgOutputNormalized.csv",index=False)
alldata.to_csv("/content/drive/MyDrive/RA-Hydrological/Sub68/AllDataAvgOutput.csv",index=False)

a=output.groupby("HRU_NUM").get_group('000680001')

a.mean()

output

output.to_csv("/content/drive/MyDrive/RA-Hydrological/Sub68/TimeWiseOutput.csv",index=False)

